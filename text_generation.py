import numpy as np
from collections import Counter, defaultdict
import re

def unzip(pairs):
    """
    "Unzips" groups of items into separate tuples.
    
    Parameters
    ----------
    pairs : Iterable[Tuple[Any, ...]]
        An iterable of the form ((a0, b0, c0, ...), (a1, b1, c1, ...))
    
    Returns
    -------
    Tuple[Tuples[Any, ...], ...]
       A tuple containing the "unzipped" contents of "pairs"; i.e. ((a0, a1, ...), (b0, b1, ...), (c0, c1), ...)
    """
    return tuple(zip(*pairs))


def normalize(counter):
    """
    Convert a "word -> count" counter to a list of (word, frequency) pairs, sorted in descending order of frequency.

    Parameters
    ----------
    counter : collections.Counter
        A counter with the format "word -> count".

    Returns
    -------
    List[Tuple[str, int]]
       A list of tuples (word, frequency) in order of descending frequency.
    """
    total = sum(counter.values())
    return [(char, cnt/total) for char, cnt in counter.most_common()]


def train_lm(text, n):
    """ 
    Train a word-based n-gram language model.

    Parameters
    ----------
    text: str 
        A string (doesn't need to be lowercase).

    n: int
        The length of n-gram to analyze.

    Returns
    -------
    Dict[str, List[Tuple[str, float]]] : {n-1 history -> [(letter, normalized count), ...]}
        A dict that maps histories (strings of length (n-1)) to lists of (word, prob) pairs,
        where "prob" is the probability/frequency of "word" appearing after that specific history.
    """
    model = defaultdict(Counter)
    
    history = "~" * (n-1)

    tokens = re.split(' |\n|\u2005', text)
    
    for word in tokens:
        model[history][word] += 1
        history = " ".join(history.split(" ")[1:]) + word
    
    lm = {history : normalize(counter) for history, counter in model.items()}
    
    return lm


def generate_word(lm, history):
    """
    Randomly picks word according to probability distribution associated with 
    the specified history, as stored in the language model.

    Parameters
    ----------
    lm: Dict[str, List[Tuple[str, float]]] 
        The n-gram language model, with format: history -> [(word, freq), ...]

    history: str
        A string of length (n-1) to use as context/history for generating the next word.

    Returns
    -------
    str
        The predicted next word, or '~' if history is not in language model.
    """
    word, prob = unzip(lm[history])
    
    if history is not None:
        return np.random.choice(word, p=prob)
    else:
        return '~'


def generate_text(lm, n, n_words=100):
    """ 
    Randomly generates n_words of text by drawing from the probability distributions stored in n-gram language model lm.

    Parameters
    ----------
    lm: Dict[str, List[Tuple[str, float]]]
        The trained n-gram language model, with format: history -> [(char, freq), ...]
        
    n: int
        Order of n-gram model.

    n_words: int
        Number of words to randomly generate.

    Returns
    -------
    str
        Text generated by the trained model.
    """
    history = "~" * (n - 1)
    
    text = []
    
    for i in range(n_words):
        word = generate_word(lm, history)
        text.append(word)
        history = " ".join(history.split(" ")[1:]) + word
        
    return " ".join(text)