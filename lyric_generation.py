import numpy as np
from collections import Counter, defaultdict
import re

def unzip(pairs):
    """
    "Unzips" groups of items into separate tuples.
    
    Parameters
    ----------
    pairs : Iterable[Tuple[Any, ...]]
        An iterable of the form ((a0, b0, c0, ...), (a1, b1, c1, ...))
    
    Returns
    -------
    Tuple[Tuples[Any, ...], ...]
       A tuple containing the "unzipped" contents of "pairs";
       i.e. ((a0, a1, ...), (b0, b1, ...), (c0, c1), ...)
    """
    return tuple(zip(*pairs))


def normalize(counter):
    """
    Convert a "word -> count" counter to a list of (word, frequency) pairs,
    sorted in descending order of frequency.

    Parameters
    ----------
    counter : collections.Counter
        A counter with the format "word -> count".

    Returns
    -------
    List[Tuple[str, int]]
       A list of tuples (word, frequency) in order of descending frequency.
    """
    total = sum(counter.values())
    return [(char, cnt/total) for char, cnt in counter.most_common()]


def train_lm(lyrics, n):
    """ 
    Train a word-based n-gram language model.

    Parameters
    ----------
    lyrics: str or List[str]
        A string or list of strings (doesn't need to be lowercase) representing the lyrics you want
        to train the model on.

    n: int
        The length of n-gram to analyze.

    Returns
    -------
    Dict[str, List[Tuple[str, float]]] : {n-1 history -> [(letter, normalized count), ...]}
        A dict that maps histories (strings of length (n-1)) to lists of (word, prob) pairs,
        where "prob" is the probability/frequency of "word" appearing after that specific history.
    """
    model = defaultdict(Counter)
    
    if type(lyrics) == list:
        tokens = []
        for item in lyrics:
            tokens.append(tuple(re.split(' |\n|\t', item)))            
    else:
        tokens = re.split(' |\n|\t', lyrics)
    
    for item in tokens:
        history = "~ " * (n-1)
        if type(item) == tuple:
            for word in item:
                if "[" in word or "]" in word or "" == word:
                    continue
                else:
                    model[history][word] += 1
                    if history[-1] == " ":
                        history = " ".join(history.split(" ")[1:]) + word
                    else:
                        history = " ".join(history.split(" ")[1:]) + " " + word
        else:
            if "[" in item or "]" in item or "" == item:
                continue
            else:
                model[history][item] += 1
                if history[-1] == " ":
                    history = " ".join(history.split(" ")[1:]) + item
                else:
                    history = " ".join(history.split(" ")[1:]) + " " + item
    
    lm = {history : normalize(counter) for history, counter in model.items()}
    
    return lm


def generate_word(lm, history):
    """
    Randomly picks word according to probability distribution associated with 
    the specified history, as stored in the language model.

    Parameters
    ----------
    lm: Dict[str, List[Tuple[str, float]]] 
        The n-gram language model, with format: history -> [(word, freq), ...]

    history: str
        A string of length (n-1) to use as context/history for generating the next word.

    Returns
    -------
    str
        The predicted next word, or '~' if history is not in language model.
    """
    word, prob = unzip(lm[history])
    
    if history is not None:
        return np.random.choice(word, p=prob)
    else:
        return '~'


def generate_text(original, lm, n, n_words=100):
    """ 
    Randomly generates n_words of text by drawing from the probability distributions stored in n-gram language model lm.

    Parameters
    ----------
    original: str or List[str]
        The original song lyrics used to train the model.
    
    lm: Dict[str, List[Tuple[str, float]]]
        The trained n-gram language model, with format: history -> [(char, freq), ...]
        
    n: int
        Order of n-gram model.

    n_words: int
        Number of words to randomly generate.

    Returns
    -------
    str
        Text generated by the trained model.
    """
    history = "~ " * (n - 1)
    
    text = []
    
    for i in range(n_words):
        word = generate_word(lm, history)
        text.append(word)
        if history[-1] == " ":
            history = " ".join(history.split(" ")[1:]) + word
        else:
            history = " ".join(history.split(" ")[1:]) + " " + word
    
    for i in range(len(text)):
        if "(" in text[i]:
            text[i] = text[i][1:]
        
    for i in range(len(text)):
        if ")" in text[i]:
            text[i] = text[i][:-1]
    
    for i in range(1, len(text)):
        if type(original) == list:
            for item in original:
                if "\n" + text[i] in item and text[i][0] != "\n":
                    text[i] = "\n" + text[i]
        else:
            if "\n" + text[i] in original:
                text[i] = "\n" + text[i]
        
    return print(" ".join(text))